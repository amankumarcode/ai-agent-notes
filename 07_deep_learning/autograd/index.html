
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../backprop/">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.16">
    
    
      
        <title>Autograd - AI-Agent Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#automatic-differentiation-autograd" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="AI-Agent Notes" class="md-header__button md-logo" aria-label="AI-Agent Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI-Agent Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Autograd
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="AI-Agent Notes" class="md-nav__button md-logo" aria-label="AI-Agent Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    AI-Agent Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Foundations
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Foundations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../00_foundations/linear_algebra/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Classic Agents
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Classic Agents
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../01_classic_agents/reflex_agents/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Simple Reflex Agents
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../01_classic_agents/model_based_reflex_agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model-Based Reflex Agents
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../02_llm_primer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Primer
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../03_transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../04_agent_patterns/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agent Patterns
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../05_multi_agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multi-Agent
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../06_rag_and_memory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG & Memory
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" checked>
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Deep Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../backprop/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backpropagation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Autograd
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Autograd
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tldr" class="md-nav__link">
    <span class="md-ellipsis">
      TL;DR
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-do-we-need-autograd" class="md-nav__link">
    <span class="md-ellipsis">
      Why Do We Need Autograd?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-core-recipe-reverse-mode" class="md-nav__link">
    <span class="md-ellipsis">
      The Core Recipe (Reverse-Mode)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#minimal-pytorch-demo" class="md-nav__link">
    <span class="md-ellipsis">
      Minimal PyTorch Demo
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-recording-works-conceptual" class="md-nav__link">
    <span class="md-ellipsis">
      How Recording Works (Conceptual)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#forward-mode-vs-reverse-mode" class="md-nav__link">
    <span class="md-ellipsis">
      Forward-Mode vs. Reverse-Mode
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-features-and-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Features and Patterns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Advanced Features and Patterns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#higher-order-gradients" class="md-nav__link">
    <span class="md-ellipsis">
      Higher-Order Gradients
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Accumulation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#selective-gradient-computation" class="md-nav__link">
    <span class="md-ellipsis">
      Selective Gradient Computation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#memory-management-and-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Management and Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Memory Management and Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-checkpointing" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Checkpointing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detaching-from-computation-graph" class="md-nav__link">
    <span class="md-ellipsis">
      Detaching from Computation Graph
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-questions-and-answers" class="md-nav__link">
    <span class="md-ellipsis">
      Common Questions and Answers
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-pitfalls-and-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      Common Pitfalls and Solutions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Common Pitfalls and Solutions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-in-place-operations" class="md-nav__link">
    <span class="md-ellipsis">
      1. In-Place Operations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-missing-requires_grad" class="md-nav__link">
    <span class="md-ellipsis">
      2. Missing requires_grad
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-memory-leaks-in-training-loops" class="md-nav__link">
    <span class="md-ellipsis">
      3. Memory Leaks in Training Loops
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-gradients-not-zeroed" class="md-nav__link">
    <span class="md-ellipsis">
      4. Gradients Not Zeroed
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#framework-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Framework Comparison
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Framework Comparison">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jax" class="md-nav__link">
    <span class="md-ellipsis">
      JAX
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensorflow-2x" class="md-nav__link">
    <span class="md-ellipsis">
      TensorFlow 2.x
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      1. Memory Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-numerical-stability" class="md-nav__link">
    <span class="md-ellipsis">
      2. Numerical Stability
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-performance-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      3. Performance Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-debugging-and-validation" class="md-nav__link">
    <span class="md-ellipsis">
      4. Debugging and Validation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../daily_logs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Daily Logs
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tldr" class="md-nav__link">
    <span class="md-ellipsis">
      TL;DR
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-do-we-need-autograd" class="md-nav__link">
    <span class="md-ellipsis">
      Why Do We Need Autograd?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-core-recipe-reverse-mode" class="md-nav__link">
    <span class="md-ellipsis">
      The Core Recipe (Reverse-Mode)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#minimal-pytorch-demo" class="md-nav__link">
    <span class="md-ellipsis">
      Minimal PyTorch Demo
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-recording-works-conceptual" class="md-nav__link">
    <span class="md-ellipsis">
      How Recording Works (Conceptual)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#forward-mode-vs-reverse-mode" class="md-nav__link">
    <span class="md-ellipsis">
      Forward-Mode vs. Reverse-Mode
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-features-and-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Features and Patterns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Advanced Features and Patterns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#higher-order-gradients" class="md-nav__link">
    <span class="md-ellipsis">
      Higher-Order Gradients
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Accumulation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#selective-gradient-computation" class="md-nav__link">
    <span class="md-ellipsis">
      Selective Gradient Computation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#memory-management-and-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Management and Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Memory Management and Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-checkpointing" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Checkpointing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detaching-from-computation-graph" class="md-nav__link">
    <span class="md-ellipsis">
      Detaching from Computation Graph
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-questions-and-answers" class="md-nav__link">
    <span class="md-ellipsis">
      Common Questions and Answers
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-pitfalls-and-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      Common Pitfalls and Solutions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Common Pitfalls and Solutions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-in-place-operations" class="md-nav__link">
    <span class="md-ellipsis">
      1. In-Place Operations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-missing-requires_grad" class="md-nav__link">
    <span class="md-ellipsis">
      2. Missing requires_grad
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-memory-leaks-in-training-loops" class="md-nav__link">
    <span class="md-ellipsis">
      3. Memory Leaks in Training Loops
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-gradients-not-zeroed" class="md-nav__link">
    <span class="md-ellipsis">
      4. Gradients Not Zeroed
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#framework-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Framework Comparison
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Framework Comparison">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jax" class="md-nav__link">
    <span class="md-ellipsis">
      JAX
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensorflow-2x" class="md-nav__link">
    <span class="md-ellipsis">
      TensorFlow 2.x
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      1. Memory Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-numerical-stability" class="md-nav__link">
    <span class="md-ellipsis">
      2. Numerical Stability
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-performance-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      3. Performance Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-debugging-and-validation" class="md-nav__link">
    <span class="md-ellipsis">
      4. Debugging and Validation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="automatic-differentiation-autograd">Automatic Differentiation (Autograd)</h1>
<p><strong>Updated:</strong> 2025-07-26</p>
<h2 id="tldr">TL;DR</h2>
<ul>
<li><strong>Goal:</strong> Get gradients <em>automatically</em>—no hand-derived math required</li>
<li><strong>Core idea:</strong> Record every elementary operation in a <em>computational graph</em> during the forward pass, then apply the chain rule backward</li>
<li><strong>Reverse-mode autograd</strong> (used for deep networks) computes all parameter gradients with roughly the cost of one extra forward pass</li>
<li><strong>Modern frameworks</strong> like PyTorch, TensorFlow 2, and JAX wrap tensors so this recording happens transparently</li>
</ul>
<hr />
<h2 id="why-do-we-need-autograd">Why Do We Need Autograd?</h2>
<blockquote>
<p>Training = tweaking millions of weights so the loss gets smaller.</p>
</blockquote>
<p>Hand-coding <span class="arithmatex">\(\frac{\partial \mathcal{L}}{\partial w}\)</span> for each layer is error-prone and impossible at GPT scale. Autograd gives us those derivatives "for free," letting us focus on model architecture and ideas rather than calculus implementation.</p>
<p><strong>The Manual Alternative Would Be:</strong>
- Deriving gradients analytically for every layer type
- Implementing backward passes for custom operations
- Debugging gradient computation errors
- Maintaining consistency as architectures evolve</p>
<p><strong>Autograd Eliminates All This:</strong> Define the forward computation, get gradients automatically.</p>
<hr />
<h2 id="the-core-recipe-reverse-mode">The Core Recipe (Reverse-Mode)</h2>
<p>The fundamental algorithm behind automatic differentiation:</p>
<ol>
<li><strong>Forward pass</strong> – compute output and <em>silently</em> build a computational graph of operations</li>
<li><strong>Seed gradient at the loss</strong> – set <span class="arithmatex">\(\frac{\partial \mathcal{L}}{\partial \mathcal{L}} = 1\)</span></li>
<li><strong>Walk graph backward</strong> – apply the chain rule to every node, caching <span class="arithmatex">\(\frac{\partial \mathcal{L}}{\partial x}\)</span> for its inputs</li>
<li><strong>Store gradients on parameters</strong> – the optimizer reads these and updates weights</li>
</ol>
<p><strong>Computational Efficiency:</strong> Because each edge is visited once forward and once backward, runtime is approximately 2× a forward pass, regardless of the number of parameters.</p>
<hr />
<h2 id="minimal-pytorch-demo">Minimal PyTorch Demo</h2>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="c1"># Define tensors with gradient tracking</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="c1"># Forward pass: graph recorded automatically</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="n">y_hat</span> <span class="o">=</span> <span class="n">W</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>        <span class="c1"># y_hat = 3*2 + 1 = 7</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="mi">7</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>    <span class="c1"># loss = (7-7)^2 = 0</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="c1"># Backward pass: autograd magic</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>          <span class="c1"># Fills .grad fields automatically</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a><span class="c1"># Access computed gradients</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;W.grad: </span><span class="si">{</span><span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># tensor([0.]) since loss is already minimized</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b.grad: </span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># tensor([0.])</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a><span class="c1"># For a more interesting example with non-zero gradients:</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">5.0</span><span class="p">])</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a><span class="n">loss2</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># loss = (7-5)^2 = 4</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a><span class="c1"># Clear previous gradients</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a><span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a><span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a><span class="n">loss2</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;W.grad: </span><span class="si">{</span><span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># tensor([8.]) = 2*(7-5)*2</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b.grad: </span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># tensor([4.]) = 2*(7-5)*1</span>
</code></pre></div>
<p><strong>Key Point:</strong> <code>requires_grad=True</code> tells PyTorch to wrap each tensor so that operations get tracked in the computational graph.</p>
<hr />
<h2 id="how-recording-works-conceptual">How Recording Works (Conceptual)</h2>
<pre class="mermaid"><code>graph LR
    X[x&lt;br/&gt;requires_grad=True] --&gt; M[multiply&lt;br/&gt;W*x]
    W[W&lt;br/&gt;requires_grad=True] --&gt; M
    M --&gt; A[add&lt;br/&gt;result + b]
    b[b&lt;br/&gt;requires_grad=True] --&gt; A
    A --&gt; S[square&lt;br/&gt;(result - target)²]
    target[target] --&gt; S
    S --&gt; L[loss&lt;br/&gt;scalar value]

    style X fill:#e1f5fe
    style W fill:#e1f5fe
    style b fill:#e1f5fe
    style L fill:#ffcdd2</code></pre>
<p><strong>Graph Node Structure:</strong>
Each node in the computational graph stores:
* <strong>Forward value</strong> computed during the forward pass
* <strong>Backward function</strong> to compute local gradients given upstream gradient
* <strong>Input references</strong> to propagate gradients backward</p>
<p><strong>Gradient Flow:</strong>
During <code>backward()</code>, gradients flow from <strong>loss</strong> → <strong>square</strong> → <strong>add</strong> → <strong>multiply</strong> → <strong>parameters</strong>.</p>
<hr />
<h2 id="forward-mode-vs-reverse-mode">Forward-Mode vs. Reverse-Mode</h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Forward-Mode</th>
<th>Reverse-Mode (Backprop)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Best for</strong></td>
<td>Few inputs, many outputs</td>
<td>Many inputs, few outputs</td>
</tr>
<tr>
<td><strong>Complexity</strong></td>
<td><span class="arithmatex">\(O(\text{num\_inputs})\)</span></td>
<td><span class="arithmatex">\(O(\text{num\_outputs})\)</span></td>
</tr>
<tr>
<td><strong>Memory</strong></td>
<td>Low</td>
<td>Higher (stores computation graph)</td>
</tr>
<tr>
<td><strong>Use case</strong></td>
<td>Jacobian-vector products</td>
<td>Neural network training</td>
</tr>
<tr>
<td><strong>Frameworks</strong></td>
<td>JAX <code>jvp</code>, PyTorch <code>forward_ad</code></td>
<td>PyTorch, TensorFlow default</td>
</tr>
</tbody>
</table>
<p><strong>Why Reverse-Mode for Deep Learning:</strong>
Neural networks typically have millions of parameters (inputs) but scalar losses (one output), making reverse-mode dramatically more efficient.</p>
<hr />
<h2 id="advanced-features-and-patterns">Advanced Features and Patterns</h2>
<h3 id="higher-order-gradients">Higher-Order Gradients</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># y = 8</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span class="c1"># First-order gradient</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="n">grad1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dy/dx = </span><span class="si">{</span><span class="n">grad1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 3*x^2 = 12</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span class="c1"># Second-order gradient (gradient of gradient)</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span class="n">grad2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">grad1</span><span class="p">,</span> <span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d²y/dx² = </span><span class="si">{</span><span class="n">grad2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 6*x = 12</span>
</code></pre></div>
<h3 id="gradient-accumulation">Gradient Accumulation</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="c1"># Useful for large batch sizes that don&#39;t fit in memory</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="k">for</span> <span class="n">mini_batch</span> <span class="ow">in</span> <span class="n">large_dataset</span><span class="p">:</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Accumulates gradients</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update with accumulated gradients</span>
</code></pre></div>
<h3 id="selective-gradient-computation">Selective Gradient Computation</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="c1"># Freeze certain parameters</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a><span class="c1"># Or temporarily disable gradients</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">validation_data</span><span class="p">)</span>  <span class="c1"># No graph building</span>
</code></pre></div>
<hr />
<h2 id="memory-management-and-optimization">Memory Management and Optimization</h2>
<h3 id="gradient-checkpointing">Gradient Checkpointing</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.checkpoint</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">checkpoint</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="k">class</span><span class="w"> </span><span class="nc">MemoryEfficientBlock</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>        <span class="c1"># Trade computation for memory</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>        <span class="k">return</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a>        <span class="c1"># Expensive computation here</span>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a>        <span class="k">return</span> <span class="n">expensive_function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<h3 id="detaching-from-computation-graph">Detaching from Computation Graph</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="c1"># Break gradient flow when needed</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="n">y</span> <span class="o">=</span> <span class="n">expensive_computation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="c1"># Use y&#39;s value but don&#39;t backprop through expensive_computation</span>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a><span class="n">z</span> <span class="o">=</span> <span class="n">some_function</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a><span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Only backprops through some_function</span>
</code></pre></div>
<hr />
<h2 id="common-questions-and-answers">Common Questions and Answers</h2>
<p><strong>Q: Why not use numerical finite differences for gradients?</strong><br />
<strong>A:</strong> Numerical methods require one forward pass per parameter (millions for large models), are computationally expensive, and suffer from numerical precision issues. Autograd computes exact gradients efficiently.</p>
<p><strong>Q: Does autograd store the whole computational graph in memory?</strong><br />
<strong>A:</strong> Yes, but you can manage memory usage with <code>torch.no_grad()</code>, <code>detach()</code>, gradient checkpointing, or by clearing graphs with <code>loss.backward(); optimizer.step(); optimizer.zero_grad()</code>.</p>
<p><strong>Q: When would I want forward-mode instead of reverse-mode?</strong><br />
<strong>A:</strong> Forward-mode is efficient for computing Jacobian-vector products when you have few inputs and many outputs. Examples include sensitivity analysis, uncertainty propagation, or computing directional derivatives.</p>
<p><strong>Q: Can I modify the computational graph during runtime?</strong><br />
<strong>A:</strong> Yes! PyTorch uses dynamic computational graphs, allowing conditional logic, loops, and runtime-dependent architectures. This enables flexible model designs and debugging.</p>
<p><strong>Q: How do I debug gradient computation?</strong><br />
<strong>A:</strong> Use gradient checking with finite differences, inspect intermediate gradients, use <code>torch.autograd.gradcheck()</code>, or visualize the computational graph with tools like <code>torch.fx</code> or <code>torchviz</code>.</p>
<hr />
<h2 id="common-pitfalls-and-solutions">Common Pitfalls and Solutions</h2>
<h3 id="1-in-place-operations">1. In-Place Operations</h3>
<p><strong>Problem:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="n">x</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># In-place operation can break autograd</span>
</code></pre></div></p>
<p><strong>Solution:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Create new tensor, preserves gradient flow</span>
</code></pre></div></p>
<h3 id="2-missing-requires_grad">2. Missing <code>requires_grad</code></h3>
<p><strong>Problem:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>  <span class="c1"># Missing requires_grad=True</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Error: no gradients to compute</span>
</code></pre></div></p>
<p><strong>Solution:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="c1"># Or: x.requires_grad_(True)  # In-place modification</span>
</code></pre></div></p>
<h3 id="3-memory-leaks-in-training-loops">3. Memory Leaks in Training Loops</h3>
<p><strong>Problem:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>  <span class="c1"># Keeps entire computation graph in memory!</span>
</code></pre></div></p>
<p><strong>Solution:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># Store only the scalar value</span>
</code></pre></div></p>
<h3 id="4-gradients-not-zeroed">4. Gradients Not Zeroed</h3>
<p><strong>Problem:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">()</span>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Gradients accumulate across epochs!</span>
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></p>
<p><strong>Solution:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Clear gradients</span>
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">()</span>
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a>    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></p>
<hr />
<h2 id="framework-comparison">Framework Comparison</h2>
<h3 id="pytorch">PyTorch</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># Dynamic graph, imperative style</span>
</code></pre></div>
<h3 id="jax">JAX</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">jax</span><span class="w"> </span><span class="kn">import</span> <span class="n">grad</span>
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a>
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a><span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a>
<a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a><span class="n">df_dx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a><span class="nb">print</span><span class="p">(</span><span class="n">df_dx</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>  <span class="c1"># Functional programming style</span>
</code></pre></div>
<h3 id="tensorflow-2x">TensorFlow 2.x</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a>
<a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
<a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a><span class="n">grad</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a><span class="nb">print</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># Explicit gradient tape</span>
</code></pre></div>
<hr />
<h2 id="best-practices">Best Practices</h2>
<h3 id="1-memory-management">1. Memory Management</h3>
<ul>
<li>Use <code>torch.no_grad()</code> for inference</li>
<li>Clear gradients regularly with <code>optimizer.zero_grad()</code></li>
<li>Consider gradient checkpointing for very deep models</li>
<li>Monitor GPU memory usage during development</li>
</ul>
<h3 id="2-numerical-stability">2. Numerical Stability</h3>
<ul>
<li>Use appropriate data types (float32 vs float64)</li>
<li>Implement gradient clipping for unstable training</li>
<li>Check for NaN gradients in training loops</li>
<li>Use numerically stable implementations of common operations</li>
</ul>
<h3 id="3-performance-optimization">3. Performance Optimization</h3>
<ul>
<li>Minimize Python loops in favor of vectorized operations</li>
<li>Use <code>torch.jit.script</code> for performance-critical code</li>
<li>Profile gradient computation with PyTorch profiler</li>
<li>Consider mixed precision training for large models</li>
</ul>
<h3 id="4-debugging-and-validation">4. Debugging and Validation</h3>
<ul>
<li>Implement gradient checking for custom operations</li>
<li>Use <code>register_hook</code> to inspect intermediate gradients</li>
<li>Validate gradients with simple test cases</li>
<li>Use deterministic operations during debugging</li>
</ul>
<hr />
<h2 id="summary">Summary</h2>
<p>Automatic differentiation is the cornerstone that makes modern deep learning practical. By automatically computing exact gradients through the chain rule, autograd systems enable:</p>
<ul>
<li><strong>Rapid prototyping</strong> of new architectures without manual gradient derivation</li>
<li><strong>Reliable gradients</strong> free from human calculation errors</li>
<li><strong>Scalable training</strong> of models with millions to billions of parameters</li>
<li><strong>Advanced techniques</strong> like meta-learning and differentiable programming</li>
</ul>
<p><strong>Key Takeaways:</strong>
1. Autograd builds computational graphs during forward passes
2. Reverse-mode is optimal for the many-parameter, few-output case of neural networks
3. Modern frameworks handle the complexity while providing fine-grained control
4. Understanding autograd principles helps debug training issues and optimize performance</p>
<p>The transition from manual gradient computation to automatic differentiation represents one of the most significant advances in making deep learning accessible and practical for complex, real-world applications.</p>
<hr />












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../backprop/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Backpropagation">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Backpropagation
              </div>
            </div>
          </a>
        
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.footer", "content.code.copy", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.50899def.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://unpkg.com/mermaid@10.6.1/dist/mermaid.min.js"></script>
      
        <script src="../../javascripts/extra.js"></script>
      
    
  </body>
</html>